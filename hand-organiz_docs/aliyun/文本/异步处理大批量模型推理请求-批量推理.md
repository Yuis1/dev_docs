---
url: https://help.aliyun.com/document_detail/2864784.html?mode=pure
title: "异步处理大批量模型推理请求-批量推理-大模型服务平台百炼-阿里云"
captured_at: "2026-01-25T01:24:40.258Z"
---


# 异步处理大批量模型推理请求-批量推理-大模型服务平台百炼-阿里云

对于无需实时响应的推理场景，批量推理（Batch API）能异步处理大批量的数据请求，成本仅为实时推理的 **50%**，且接口兼容 OpenAI，适合执行模型评测、数据标注等批量作业。

## 工作流程

批量推理采用异步模式：

1. **提交任务：**上传包含多个请求的文件，创建一个批量推理任务。
2. **异步处理：**系统在后台处理队列中的任务。任务进度和状态可在控制台或通过 API 查询。
3. **下载结果：**任务完成后，系统会生成一个包含成功响应的结果文件和一个包含失败详情的错误文件（如有）。

## 适用范围

中国内地

国际

**


**


**支持的模型**：

- **文本生成模型**：通义千问 Max、Plus、Flash、Turbo、Long 的稳定版本及其部分 `latest` 版本，以及 QwQ 系列（qwq-plus、qwq-32b-preview）和部分第三方模型（deepseek-r1、deepseek-v3）。
- **多模态模型**：通义千问 VL Max、Plus、Flash、OCR的稳定版本及其部分 `latest` 版本，以及通义千问 Omni 模型。
- **文本向量模型**：所有版本的 text-embedding 模型。


支持的模型名称清单

 **


- **文本生成模型**[通义千问 Max](https://help.aliyun.com/zh/model-studio/models#cfc131abafghw)：qwen3-max、qwen-max、qwen-max-latest
- [通义千问 Plus](https://help.aliyun.com/zh/model-studio/models#6c45e49509gtr)：qwen-plus、qwen-plus-latest
- [通义千问 Flash](https://help.aliyun.com/zh/model-studio/models#d617df95f1g9h)：qwen-flash
- [通义千问 Turbo](https://help.aliyun.com/zh/model-studio/models#8708390fdb66x)：qwen-turbo、qwen-turbo-latest
- [通义千问 Long](https://help.aliyun.com/zh/model-studio/models#27b2b3a15d5c6)：qwen-long、qwen-long-latest
- [QwQ](https://help.aliyun.com/zh/model-studio/models#874b221f2cx9k)：qwq-plus
- [QwQ-Preview](https://help.aliyun.com/zh/model-studio/models#18e88cc886ll3)：qwq-32b-preview
- 第三方模型：deepseek-r1、deepseek-v3

**多模态模型**

- [视觉理解](https://help.aliyun.com/zh/model-studio/vision)：qwen3-vl-plus、qwen3-vl-flash、qwen-vl-max、qwen-vl-max-latest、qwen-vl-plus、qwen-vl-plus-latest
- [文字提取](https://help.aliyun.com/zh/model-studio/qwen-vl-ocr)：qwen-vl-ocr、qwen-vl-ocr-latest
- [全模态](https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni)：qwen-omni-turbo

[**文本向量模型**](https://help.aliyun.com/zh/model-studio/user-guide/embedding)**：**text-embedding-v1、text-embedding-v2、text-embedding-v3、text-embedding-v4


**支持的模型**：qwen-max、qwen-plus、qwen-turbo。

## 如何使用

### 步骤一：准备输入文件

创建批量推理任务前，需先准备一个符合以下规范的文件：

- **格式**：UTF-8 编码的 JSONL（每行一个独立 JSON 对象）。
- **规模限制**：单文件 ≤ 50,000 个请求，且 ≤ 500 MB。**如果数据量超过此限制，建议拆分任务分别提交。单行限制**：每个 JSON 对象 ≤ 6 MB，且不超过模型上下文长度。
- **一致性要求**：同一文件内所有请求须使用相同模型及思考模式（如适用）。
- **唯一标识**：每个请求必须包含文件内唯一的 custom_id 字段，用于结果匹配。

#### 请求示例

可下载示例文件[test_model.jsonl](https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20250926/wrheek/test_model.jsonl)，内容为：


**
 **


```
{"custom_id":"1","method":"POST","url":"/v1/chat/completions","body":{"model":"qwen-max","messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"你好！有什么可以帮助你的吗？"}]}}
{"custom_id":"2","method":"POST","url":"/v1/chat/completions","body":{"model":"qwen-max","messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"What is 2+2?"}]}}

```


**JSONL 批量生成工具**

使用以下工具可快速生成 JSONL 文件。


 JSONL 批量生成工具


**请选择模式：**


**中国内地
 国际


 **选择模型系列:**
 文本生成模型 多模态模型 通用文本向量模型


 **选择具体模型:**
 qwen3-max qwen-max qwen-max-latest qwen-flash（思考模式） qwen-flash（非思考模式） qwen-plus（思考模式） qwen-plus（非思考模式） qwen-plus-latest（思考模式） qwen-plus-latest（非思考模式） qwen-turbo（思考模式） qwen-turbo（非思考模式） qwen-turbo-latest（思考模式） qwen-turbo-latest（非思考模式） qwen-long qwen-long-latest qwq-plus qwq-32b-preview deepseek-r1 deepseek-v3


 **写入您的请求内容（每行一条请求）:**
 你好！有什么可以帮助你的吗？
What is 2+2?


 **粘贴您的媒体URL (每行一个或多个，英文逗号分隔):**

 **输入您对媒体的提问:**


 生成


 **请选择模式：**


 中国内地
 国际


 **选择模型系列:**
 文本生成模型


 **选择具体模型:**
 qwen-max qwen-plus qwen-turbo


 **写入您的请求内容（每行一条请求）:**
 你好！有什么可以帮助你的吗？
What is 2+2?


 生成


### 步骤二：提交并查看结果

可通过控制台或 API 创建并管理任务。控制台API


#### （一）创建批量推理任务

在批量推理**页面，单击**创建批量推理任务**。

在弹出的对话框中：填写**任务名称**和**描述**、设置**最长等待时间**（1-14天），上传JSONL 文件。

**可点击下载示例文件**获取模板。

*填写完成后，单击**确认**。

#### （二）查看与管理任务

**查看**：在任务列表页，可查看任务的**进度**（已处理请求数/总请求数）和**状态**。支持按任务名称/ID搜索或按业务空间筛选，以快速定位任务。**管理**：取消：“执行中”的任务可在**操作**列取消。排错：“失败”的任务可通过悬停状态查看概要，下载错误文件查看详情。

#### （三）下载并分析结果

任务完成后，单击**查看结果**，可下载产出文件：**结果文件**： 记录所有成功的请求及其 response 结果。**错误文件（如有）**： 记录所有失败的请求及其 error 详情。两个文件均包含 custom_id 字段，用于与原始输入数据进行匹配，从而关联结果或定位错误。对于需要自动化和集成的生产环境，推荐使用与 OpenAI 兼容的 Batch API，核心流程为：**创建任务**
调用 POST /v1/batches 接口创建任务，并记录返回的 batch_id。**轮询状态**
使用 batch_id 轮询 GET /v1/batches/{batch_id} 接口。当 status 字段变为 completed 时，记录返回的 output_file_id 并停止轮询。**下载结果**
使用 output_file_id 调用 GET /v1/files/{output_file_id}/content 接口，即可下载结果文件。**获取完整的接口定义、参数信息和代码示例，请参阅OpenAI兼容-Batch。

### 步骤三：查看数据统计（可选）

在模型观测页面，可筛选并查看批量推理的用量统计。查看数据概览**： 选择要查询的**时间范围**（最长支持30天），将**推理类型**选为**批量推理**，即可查看：**监控数据**：显示该时间段内所有模型的汇总统计，如总调用次数、总失败次数等。**模型列表**：逐一列出每个模型的详细数据，如调用总量、失败率、平均调用时长等。**如需查看 30 天前的推理数据，可前往账单页面查询。查看模型详情**： 在**模型列表**中，单击目标模型右侧**操作**列的**监控**，可查看该模型的**调用统计**（如调用次数、调用量等）详情。***重要**

- 批量推理的调用数据以**任务结束时间**为准进行统计。对于正在运行的任务，其调用信息在任务完成前无法查询到。
- 监控数据存在1～2小时延迟。

## 任务生命周期

- **validating（验证中）：**系统正在校验上传的数据文件格式是否符合 JSONL 规范，以及文件内的每一行请求是否符合 API 格式要求。
- **in_progress（执行中）：**文件验证通过，系统已开始逐行处理文件中的推理请求。
- **completed（已完成）：**结果文件和错误文件数据已写入完成，可下载。
- **failed（失败）：**任务在 validating 状态失败，通常由文件级错误（如 JSONL 格式错误、文件过大）导致。此状态下，任务不会执行任何推理请求，也不会产生结果文件。
- **expired（已终止）：**任务因运行时间超过创建时设定的最长等待时间而被系统终止。如果任务因此失败，建议在创建新任务时设置更长的等待时间。
- **cancelled（已取消）：**任务已取消。任务中未开始处理的请求将被终止。

## 计费说明

- **计费单价：**所有成功请求的输入和输出Token，单价均为对应模型实时推理价格的**50%**，具体请参见[模型列表](https://help.aliyun.com/zh/model-studio/models#9f8890ce29g5u)。
- **计费范围：**仅对任务中成功执行的请求进行计费。
- 文件解析失败、任务执行失败、或行级错误请求均**不产生费用**。
- 对于被取消的任务，在取消操作前已成功完成的请求仍会正常计费。

****重要**批量推理为独立计费项，不支持[预付费](https://common-buy.aliyun.com/?commodityCode=sfm_llminference_spn_public_cn)（节省计划）、[新人免费额度](https://help.aliyun.com/zh/model-studio/new-free-quota)等优惠，以及[上下文缓存](https://help.aliyun.com/zh/model-studio/context-cache)等功能。

## 常见问题

1. **使用批量推理需要额外购买或开通吗？**不需要。只要已开通阿里云百炼服务。费用将按**后付费模式**从账户余额中扣除。
2. **任务提交后为什么立即失败（状态变为 **`**failed**`**）？**这通常是文件级错误导致的，任务并未开始执行任何推理请求。请按以下顺序排查：

- **文件格式**：是否为严格的 JSONL 格式，每行一个完整的 JSON 对象。
- **文件规模：**文件大小、行数等是否超出限制。详情请参见[准备输入文件](#cdb5ab7b74k2t)。
- **模型一致性：**检查文件中所有请求的 `body.model` 字段是否完全一致，且使用的是当前地域支持的模型。
3. **任务处理需要多长时间？**任务处理时长主要取决于系统当时的负载，当系统繁忙时，任务可能需要排队等待资源，成功或失败都会在设定的“最长等待时间”内返回结果。

## 错误码

如果调用失败并返回报错信息，请参见[错误信息](https://help.aliyun.com/zh/model-studio/error-code)进行解决。
